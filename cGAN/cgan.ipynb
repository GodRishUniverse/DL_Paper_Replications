{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cGAN (conditional GAN) implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Convolutional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size_z = 100, input_size_condition= 100, hidden_size = 256, output_size = 784, layers = 1,leaky = 0.2, device = 'cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.init_layer = nn.Sequential(\n",
    "            nn.Linear(input_size_z +input_size_condition, 800, device = self.device),\n",
    "            nn.LeakyReLU(leaky),\n",
    "        )\n",
    "\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(800, hidden_size, device = self.device),\n",
    "            nn.LeakyReLU(leaky),\n",
    "        )\n",
    "\n",
    "        self.layer = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size, device = self.device),\n",
    "            nn.LeakyReLU(leaky),\n",
    "        ) for _ in range(layers-1)])\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size, device = self.device),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        \"\"\"\n",
    "        z: the vector \n",
    "        y: the label for the vector\n",
    "        \"\"\"\n",
    "        z = z.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        combined = self.init_layer(torch.cat((z, y), dim=1))\n",
    "        combined = self.combine(combined)\n",
    "\n",
    "        for layer in self.layer:\n",
    "            combined = layer(combined)\n",
    "            \n",
    "        return self.final(combined) # logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator test passed\n"
     ]
    }
   ],
   "source": [
    "def test_generator():\n",
    "    gen = Generator(input_size_z=100, input_size_condition=10, output_size=784, layers=3, device='cuda')\n",
    "    noise = torch.randn(1, 100)\n",
    "    label = torch.randn(1, 10)\n",
    "\n",
    "    output = gen(noise, label)\n",
    "    assert output.shape == (1, 784)\n",
    "    print(\"Generator test passed\")\n",
    "test_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator - not using maxout because of instability issues\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d_in =784, d_label = 10, hidden_size=256, d_out =1, leaky = 0.2, dropout = 0.5, device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.label_embed = nn.Linear(d_label, hidden_size, device = self.device)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(d_in + hidden_size, hidden_size, device = self.device),\n",
    "            nn.LeakyReLU(leaky),\n",
    "            nn.Linear(hidden_size, hidden_size, device = self.device),\n",
    "            nn.LeakyReLU(leaky),\n",
    "            nn.Linear(hidden_size, d_out, device = self.device)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        y = y.to(self.device, dtype=torch.float32)\n",
    "        \n",
    "        y = self.label_embed(y)\n",
    "\n",
    "        combined = torch.cat((x,y), dim =1)\n",
    "\n",
    "        logits = self.dropout(self.model(combined))\n",
    "\n",
    "        return logits # torch.sigmoid(combined) or torch.tanh(combined) - used with loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 784]) torch.Size([1, 10]) torch.float32 torch.float32\n",
      "Discriminator test passed\n"
     ]
    }
   ],
   "source": [
    "# Unit testing for Discriminator\n",
    "\n",
    "def test_discriminator():\n",
    "   disc = Discriminator(d_in = 784, d_label = 10, hidden_size=256, d_out =1, leaky = 0.2, dropout = 0.5, device='cuda')\n",
    "   x = torch.randn(1, 784)\n",
    "   y = torch.randn(1, 10)\n",
    "\n",
    "   output = disc(x, y)\n",
    "   assert output.shape == (1, 1)\n",
    "   print(\"Discriminator test passed\")\n",
    "test_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "DISC_LR= 0.001\n",
    "GEN_LR= 0.1\n",
    "MIN_LR = 0.000001\n",
    "DECAY_FACTOR = 1.00004\n",
    "DROPOUT = 0.5\n",
    "INIT_MOMENTUM = 0.5\n",
    "MAX_MOMENTUM = 0.7\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 100\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, device):\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        # x is the real data\n",
    "        # y is the labels\n",
    "\n",
    "        x = x.view(x.shape[0], -1).to(device)\n",
    "        y = y.unsqueeze(1).to(device)\n",
    "\n",
    "\n",
    "        optim_gen.zero_grad()\n",
    "        optim_disc.zero_grad()\n",
    "        \n",
    "        z = torch.randn(x.shape[0], 784).to(device)\n",
    "        gen_out = gen(z, y)\n",
    "\n",
    "        # Discriminator predictions\n",
    "        disc_out_real = disc(x, y)  # D(real)\n",
    "        disc_out_fake = disc(gen_out.detach(), y)  # D(fake), detach G to avoid gradient flow to Generator\n",
    "\n",
    "        # Create real and fake labels\n",
    "        real_labels = torch.ones_like(disc_out_real)\n",
    "        fake_labels = torch.zeros_like(disc_out_fake)\n",
    "\n",
    "        # Compute Discriminator loss\n",
    "        loss_disc_real = criterion_disc(disc_out_real, real_labels)  # D(x) should be 1\n",
    "        loss_disc_fake = criterion_disc(disc_out_fake, fake_labels)  # D(G(z)) should be 0\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "        loss_disc.backward()\n",
    "        optim_disc.step()\n",
    " \n",
    "        # Train Generator (G)\n",
    "        # Recalculate fake images (since .detach() was used before)\n",
    "        gen_out = gen(z, y)\n",
    "        disc_out_fake = disc(gen_out, y)  # D(G(z)), should be classified as real\n",
    "\n",
    "        # Compute Generator loss\n",
    "        real_labels = torch.ones_like(disc_out_fake)  # Generator wants D to classify as real\n",
    "        loss_gen = criterion_gen(disc_out_fake, real_labels)\n",
    "\n",
    "        loss_gen.backward()\n",
    "        optim_gen.step()\n",
    "     \n",
    "        scheduler_gen.step()\n",
    "        scheduler_disc.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(gen, disc, criterion_gen, criterion_disc, test_loader, device):\n",
    "    gen.eval()\n",
    "    disc.eval()\n",
    "    with torch.inference_mode():\n",
    "        sample_gen = None\n",
    "        disc_loss = 0 \n",
    "        gen_loss = 0\n",
    "        for x, y in test_loader:\n",
    "            x = x.view(x.shape[0], -1).to(device)\n",
    "            y = y.unsqueeze(1).to(device)\n",
    "\n",
    "            z = torch.randn(x.shape[0], 784).to(device)\n",
    "            gen_out = gen(z, y)\n",
    "\n",
    "            disc_out_real = disc(x, y)\n",
    "            disc_out_fake = disc(gen_out, y)\n",
    "\n",
    "            real_labels = torch.ones_like(disc_out_real)\n",
    "            fake_labels = torch.zeros_like(disc_out_fake)\n",
    "\n",
    "            loss_disc_real = criterion_disc(disc_out_real, real_labels)\n",
    "            loss_disc_fake = criterion_disc(disc_out_fake, fake_labels)\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "            loss_gen = criterion_gen(disc_out_fake, real_labels)\n",
    "\n",
    "            disc_loss += loss_disc.item()\n",
    "            gen_loss += loss_gen.item()\n",
    "\n",
    "            \n",
    "            sample_gen  = torch.stack([x, gen_out], dim=1)\n",
    "        \n",
    "        return sample_gen, disc_loss/len(test_loader), gen_loss/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(sample_gen):\n",
    "    sample_gen = sample_gen.reshape(-1, 1, 28, 28)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Images\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(sample_gen, padding=2, normalize=True).cpu(), (1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, test_loader, epochs, device):\n",
    "    for i in tqdm(range(epochs)):\n",
    "        train(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, device)\n",
    "        sample_gen, disc_loss, gen_loss = test(gen, disc, criterion_gen, criterion_disc, test_loader, device)\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Epoch: {i}, Discriminator loss: {disc_loss}, Generator loss: {gen_loss}\") \n",
    "\n",
    "    visualize(sample_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./train_data', train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./test_data', train=False, download=True, transform=transforms.ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders \n",
    "train_loader, test_loader = DataLoader(train_dataset,batch_size = BATCH_SIZE, shuffle= True), DataLoader(test_dataset,batch_size = BATCH_SIZE, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1721e7c5994dbf8f6335d8b3669386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Discriminator loss: 0.6941260331869126, Generator loss: 0.6868000757694245\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m gen \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m disc \u001b[38;5;241m=\u001b[39m disc\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[1;34m(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, test_loader, epochs, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_test\u001b[39m(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, test_loader, epochs, device):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m----> 3\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_disc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m         sample_gen, disc_loss, gen_loss \u001b[38;5;241m=\u001b[39m test(gen, disc, criterion_gen, criterion_disc, test_loader, device)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[74], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, device)\u001b[0m\n\u001b[0;32m     41\u001b[0m real_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(disc_out_fake)  \u001b[38;5;66;03m# Generator wants D to classify as real\u001b[39;00m\n\u001b[0;32m     42\u001b[0m loss_gen \u001b[38;5;241m=\u001b[39m criterion_gen(disc_out_fake, real_labels)\n\u001b[1;32m---> 44\u001b[0m \u001b[43mloss_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m optim_gen\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     47\u001b[0m scheduler_gen\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Rishabh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rishabh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen = Generator(input_size_z=784, input_size_condition=1, output_size=784, hidden_size=256, layers=3, device=device)\n",
    "disc = Discriminator(d_in = 784, d_label = 1, hidden_size=256, d_out =1, leaky = 0.2, dropout = DROPOUT,device=device)\n",
    "\n",
    "optim_gen = optim.SGD(gen.parameters(), lr=GEN_LR, momentum=INIT_MOMENTUM, weight_decay=0.0001)\n",
    "optim_disc = optim.SGD(disc.parameters(), lr=DISC_LR, momentum=INIT_MOMENTUM,weight_decay=0.01)\n",
    "\n",
    "scheduler_gen = optim.lr_scheduler.ExponentialLR(optim_gen, DECAY_FACTOR) \n",
    "scheduler_disc = optim.lr_scheduler.ExponentialLR(optim_disc, DECAY_FACTOR)\n",
    "\n",
    "criterion_gen = nn.BCEWithLogitsLoss() # we use the raw logits\n",
    "criterion_disc = nn.BCEWithLogitsLoss() # we use the raw logits\n",
    "\n",
    "\n",
    "gen = gen.to(device)\n",
    "disc = disc.to(device)\n",
    "\n",
    "train_and_test(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, test_loader, EPOCHS, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
