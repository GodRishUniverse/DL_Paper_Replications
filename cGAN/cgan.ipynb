{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cGAN (conditional GAN) implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Convolutional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size_z = 100, input_size_condition= 100, hidden_size = 256, output_size = 784, layers = 1, device = 'cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.init_layer = nn.Sequential(\n",
    "            nn.Linear(input_size_z +input_size_condition, 1200, device = self.device),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(1200, hidden_size, device = self.device),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size, device = self.device),\n",
    "            nn.ReLU(),\n",
    "        ) for _ in range(layers-1)])\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size, device = self.device),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        \"\"\"\n",
    "        z: the vector \n",
    "        y: the label for the vector\n",
    "        \"\"\"\n",
    "        z = z.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        combined = self.init_layer(torch.cat((z, y), dim=1))\n",
    "        combined = self.combine(combined)\n",
    "\n",
    "        for layer in self.layer:\n",
    "            combined = layer(combined)\n",
    "            \n",
    "        return self.final(combined) # logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator test passed\n"
     ]
    }
   ],
   "source": [
    "def test_generator():\n",
    "    gen = Generator(input_size_z=100, input_size_condition=10, output_size=784, layers=3, device='cuda')\n",
    "    noise = torch.randn(1, 100)\n",
    "    label = torch.randn(1, 10)\n",
    "\n",
    "    output = gen(noise, label)\n",
    "    assert output.shape == (1, 784)\n",
    "    print(\"Generator test passed\")\n",
    "test_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator - not using maxout because of instability issues\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d_in =784, d_label = 10, hidden_size=256, d_out =1, leaky = 0.2, dropout = 0.5, device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.label_embed = nn.Linear(d_label, hidden_size, device = self.device)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(d_in + hidden_size, hidden_size, device = self.device),\n",
    "            nn.LeakyReLU(leaky),\n",
    "            nn.Linear(hidden_size, hidden_size, device = self.device),\n",
    "            nn.LeakyReLU(leaky),\n",
    "            nn.Linear(hidden_size, d_out, device = self.device)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        y = self.label_embed(y)\n",
    "\n",
    "        combined = torch.cat((x,y), dim =1)\n",
    "\n",
    "        logits = self.dropout(self.model(combined))\n",
    "\n",
    "        return logits # torch.sigmoid(combined) or torch.tanh(combined) - used with loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator test passed\n"
     ]
    }
   ],
   "source": [
    "# Unit testing for Discriminator\n",
    "\n",
    "def test_discriminator():\n",
    "   disc = Discriminator(d_in = 784, d_label = 10, hidden_size=256, d_out =1, leaky = 0.2, dropout = 0.5, device='cuda')\n",
    "   x = torch.randn(1, 784)\n",
    "   y = torch.randn(1, 10)\n",
    "\n",
    "   output = disc(x, y)\n",
    "   assert output.shape == (1, 1)\n",
    "   print(\"Discriminator test passed\")\n",
    "test_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "LR= 0.1\n",
    "MIN_LR = 0.000001\n",
    "DECAY_FACTOR = 1.00004\n",
    "DROPOUT = 0.5\n",
    "INIT_MOMENTUM = 0.5\n",
    "MAX_MOMENTUM = 0.7\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(input_size_z=100, input_size_condition=10, output_size=784, layers=3, device=device)\n",
    "disc = Discriminator(d_in = 784, d_label = 10, hidden_size=256, d_out =1, leaky = 0.2, dropout = DROPOUT,device=device)\n",
    "\n",
    "optim_gen = optim.SGD(gen.parameters(), lr=LR, momentum=INIT_MOMENTUM,)\n",
    "optim_disc = optim.SGD(disc.parameters(), lr=LR, momentum=INIT_MOMENTUM)\n",
    "\n",
    "scheduler_gen = optim.lr_scheduler.ExponentialLR(optim_gen, DECAY_FACTOR) \n",
    "scheduler_disc = optim.lr_scheduler.ExponentialLR(optim_disc, DECAY_FACTOR)\n",
    "\n",
    "criterion_gen = nn.BCELossWithLogitsLoss() # we use the raw logits\n",
    "criterion_disc = nn.BCELossWithLogitsLoss() # we use the raw logits\n",
    "\n",
    "\n",
    "gen = gen.to(device)\n",
    "disc = disc.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, device):\n",
    "    gen.train()\n",
    "    disc.train()\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        # x is the real data\n",
    "        # y is the labels\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optim_gen.zero_grad()\n",
    "        optim_disc.zero_grad()\n",
    "        \n",
    "        z = torch.randn(x.shape[0], 100).to(device)\n",
    "        gen_out = gen(z, y)\n",
    "\n",
    "        # Discriminator predictions\n",
    "        disc_out_real = disc(x, y)  # D(real)\n",
    "        disc_out_fake = disc(gen_out.detach(), y)  # D(fake), detach G to avoid gradient flow to Generator\n",
    "\n",
    "        # Create real and fake labels\n",
    "        real_labels = torch.ones_like(disc_out_real)\n",
    "        fake_labels = torch.zeros_like(disc_out_fake)\n",
    "\n",
    "        # Compute Discriminator loss\n",
    "        loss_disc_real = criterion_disc(disc_out_real, real_labels)  # D(x) should be 1\n",
    "        loss_disc_fake = criterion_disc(disc_out_fake, fake_labels)  # D(G(z)) should be 0\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "        loss_disc.backward()\n",
    "        optim_disc.step()\n",
    " \n",
    "        # Train Generator (G)\n",
    "        # Recalculate fake images (since .detach() was used before)\n",
    "        gen_out = gen(z, y)\n",
    "        disc_out_fake = disc(gen_out, y)  # D(G(z)), should be classified as real\n",
    "\n",
    "        # Compute Generator loss\n",
    "        real_labels = torch.ones_like(disc_out_fake)  # Generator wants D to classify as real\n",
    "        loss_gen = criterion_gen(disc_out_fake, real_labels)\n",
    "\n",
    "        loss_gen.backward()\n",
    "        optim_gen.step()\n",
    "\n",
    "        scheduler_gen.step()\n",
    "        scheduler_disc.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(gen, disc, criterion_gen, criterion_disc, test_loader, device):\n",
    "    gen.eval()\n",
    "    disc.eval()\n",
    "    with torch.inference_mode():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            z = torch.randn(x.shape[0], 100).to(device)\n",
    "            gen_out = gen(z, y)\n",
    "\n",
    "            disc_out_real = disc(x, y)\n",
    "            disc_out_fake = disc(gen_out, y)\n",
    "\n",
    "            real_labels = torch.ones_like(disc_out_real)\n",
    "            fake_labels = torch.zeros_like(disc_out_fake)\n",
    "\n",
    "            loss_disc_real = criterion_disc(disc_out_real, real_labels)\n",
    "            loss_disc_fake = criterion_disc(disc_out_fake, fake_labels)\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "            loss_gen = criterion_gen(disc_out_fake, real_labels)\n",
    "\n",
    "            print(f\"Discriminator loss: {loss_disc.item():.4f} | Generator loss: {loss_gen.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, test_loader, epochs, device):\n",
    "    for _ in range(epochs):\n",
    "        train(gen, disc, optim_gen, optim_disc, scheduler_gen, scheduler_disc, criterion_gen, criterion_disc, train_loader, device)\n",
    "        test(gen, disc, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
