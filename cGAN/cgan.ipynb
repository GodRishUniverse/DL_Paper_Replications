{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cGAN (conditional GAN) implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size_z = 100, input_size_condition= 100, hidden_size = 256, output_size = 784, layers = 1, device = 'cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.init_layer = nn.Sequential(\n",
    "            nn.Linear(input_size_z +input_size_condition, 1200, device = self.device),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(1200, hidden_size, device = self.device),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size, device = self.device),\n",
    "            nn.ReLU(),\n",
    "        ) for _ in range(layers-1)])\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size, device = self.device),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        \"\"\"\n",
    "        z: the vector \n",
    "        y: the label for the vector\n",
    "        \"\"\"\n",
    "        z = z.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        combined = self.init_layer(torch.cat((z, y), dim=1))\n",
    "        combined = self.combine(combined)\n",
    "\n",
    "        for layer in self.layer:\n",
    "            combined = layer(combined)\n",
    "            \n",
    "        return self.final(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator test passed\n"
     ]
    }
   ],
   "source": [
    "def test_generator():\n",
    "    gen = Generator(input_size_z=100, input_size_condition=10, output_size=784, layers=3, device='cuda')\n",
    "    noise = torch.randn(1, 100)\n",
    "    label = torch.randn(1, 10)\n",
    "\n",
    "    output = gen(noise, label)\n",
    "    assert output.shape == (1, 784)\n",
    "    print(\"Generator test passed\")\n",
    "test_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator - not using maxout because of instability issues\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d_in =784, d_label = 10, hidden_size=256, d_out =1, leaky = 0.2, dropout = 0.5, device='cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.label_embed = nn.Linear(d_label, hidden_size, device = self.device)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(d_in + hidden_size, hidden_size, device = self.device),\n",
    "            nn.LeakyReLU(leaky),\n",
    "            nn.Linear(hidden_size, hidden_size, device = self.device),\n",
    "            nn.LeakyReLU(leaky),\n",
    "            nn.Linear(hidden_size, d_out, device = self.device)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        y = self.label_embed(y)\n",
    "\n",
    "        combined = torch.cat((x,y), dim =1)\n",
    "\n",
    "        logits = self.dropout(self.model(combined))\n",
    "\n",
    "        return logits # torch.sigmoid(combined) or torch.tanh(combined) - used with loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator test passed\n"
     ]
    }
   ],
   "source": [
    "# Unit testing for Discriminator\n",
    "\n",
    "def test_discriminator():\n",
    "   disc = Discriminator(d_in = 784, d_label = 10, hidden_size=256, d_out =1, leaky = 0.2, dropout = 0.5, device='cuda')\n",
    "   x = torch.randn(1, 784)\n",
    "   y = torch.randn(1, 10)\n",
    "\n",
    "   output = disc(x, y)\n",
    "   assert output.shape == (1, 1)\n",
    "   print(\"Discriminator test passed\")\n",
    "test_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "LR= 0.1\n",
    "MIN_LR = 0.000001\n",
    "DECAY_FACTOR = 1.00004\n",
    "DROPOUT = 0.5\n",
    "INIT_MOMENTUM = 0.5\n",
    "MAX_MOMENTUM = 0.7\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
