{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cGAN (conditional GAN) implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the Todo-list\n",
    "Will be updated as we go about the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size_z = 100, input_size_condition= 100, hidden_size = 256, output_size = 784, layers = 1, device = 'cuda'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Linear(input_size_z, 200),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.Linear(input_size_condition, 1000),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(1200, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer = nn.ModuleList([nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        ) for _ in range(layers-1)])\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        \"\"\"\n",
    "        z: the vector \n",
    "        y: the label for the vector\n",
    "        \"\"\"\n",
    "        z = self.layer_1(z)\n",
    "        y = self.layer_2(y)\n",
    "\n",
    "        combined = torch.cat((z, y), 1)\n",
    "        combined = self.combine(combined)\n",
    "\n",
    "        for layer in self.layer:\n",
    "            combined = layer(combined)\n",
    "            \n",
    "        return self.final(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator test passed\n"
     ]
    }
   ],
   "source": [
    "def test_generator():\n",
    "    gen = Generator(input_size_z=100, input_size_condition=10, output_size=784, layers=3, device='cuda')\n",
    "    noise = torch.randn(1, 100)\n",
    "    label = torch.randn(1, 10)\n",
    "\n",
    "    output = gen(noise, label)\n",
    "    assert output.shape == (1, 784)\n",
    "    print(\"Generator test passed\")\n",
    "test_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: To implement\n",
    "class MaxOut(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        #TODO: look into maxout layer\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
