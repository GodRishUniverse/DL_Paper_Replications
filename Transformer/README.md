# Transformer

Adapted from the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper.

You can find the `.pdf` file of the paper from the repo
