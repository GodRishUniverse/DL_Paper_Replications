{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add dropout and fix the mask code\n",
    "#TODO: implement the model\n",
    "#TODO: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Rishabh Agarwal\n",
    "# All the imports for the code\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "# Used in the Transformer Block\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model=512):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiheaded attention class for the transformer\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_k, d_model, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.d_k = d_k # d_k = d_v\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        assert d_model % h == 0  # Assert that the number of heads divides the model dimension\n",
    "        assert d_k == d_model // h # Assert that the key and value dimensions are equal to d_model // h\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k, device = device)\n",
    "        self.W_K = nn.Linear(d_model, d_k, device = device)\n",
    "        self.W_V = nn.Linear(d_model, d_k, device = device)\n",
    "\n",
    "        self.W_O = nn.Linear(h*d_k, d_model, device = device)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: (batch_size, seq_len, d_model)\n",
    "        # mask: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        concatenated_heads = torch.tensor([], device=self.device)\n",
    "\n",
    "        # The multiple heads being computed to be concatenated together and then applied to the output layer\n",
    "        # can be done in parallel\n",
    "        for _ in range(self.h):\n",
    "            # Linearly project the queries\n",
    "            computed_Q = self.W_Q(Q)\n",
    "            # Linearly project the keys\n",
    "            computed_K = self.W_K(K)\n",
    "            # Linearly project the queries, keys, and values\n",
    "            computed_V = self.W_V(V)\n",
    "\n",
    "            # Calculate the attention scores\n",
    "            head_i = F.scaled_dot_product_attention(computed_Q, computed_K, computed_V, mask)\n",
    "            # print(head_i.shape)\n",
    "            concatenated_heads = torch.cat((concatenated_heads, head_i), dim=-1)\n",
    "            \n",
    "        return self.W_O(concatenated_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the MultiHeadedAttention class\n",
    "def test_multi_headed_attention(device):\n",
    "    h = 8\n",
    "    d_k = 64\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    mha = MultiHeadedAttention(h, d_k, d_model,device)\n",
    "\n",
    "    Q = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    K = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    V = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = mha(Q, K, V)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"MultiHeadedAttention test passed\")\n",
    "test_multi_headed_attention(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
