{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the Todo-list\n",
    "Will be updated as we go about the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: figure out inference for transformer model - use of self.training\n",
    "#TODO: look into torchtext\n",
    "#TODO: get the data\n",
    "#TODO: pre-process the data - tokenize it as well\n",
    "#TODO: implement the training loop\n",
    "#TODO: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Rishabh Agarwal\n",
    "# All the imports for the code\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "# Used in the Transformer Block\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, device, d_model=512, dff = 2048):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dff, device=device)\n",
    "        self.linear2 = nn.Linear(dff, d_model, device=device)\n",
    "        self.gelu = nn.GELU()\n",
    "        # One reason of choosing GELU - not in the initial implementation of the transformer in the paper\n",
    "        \"\"\"\n",
    "        relu can suffer from \"problems where significant amount of neuron in the network become zero and donâ€™t practically do anything.\" \n",
    "        gelu is smoother near zero and \"is differentiable in all ranges, and allows to have gradients(although small) in negative range\" which helps with this problem.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dropout=0.1, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.device = device\n",
    "        self.d_k = d_model // num_heads # using d_model and number of heads for d_k calculation\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Combined linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(d_model, d_model,device=self.device)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, device=self.device)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, device=self.device)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, device= self.device)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections and reshape for attention\n",
    "        q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_proj(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_proj(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention (can use internal implementation as well)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attn, v)\n",
    "        \n",
    "        # Reshape and apply output projection\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.out_proj(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the MultiHeadedAttention class\n",
    "def test_multi_headed_attention(device):\n",
    "    h = 8\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    mha = MultiHeadAttention(num_heads=h, d_model= d_model, device=device)\n",
    "\n",
    "    Q = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    K = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    V = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = mha(Q, K, V)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"MultiHeadedAttention test passed\")\n",
    "test_multi_headed_attention(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Encoder Blocks in the Transformer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads, d_model, dropout, device=device)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model, device=device)\n",
    "\n",
    "        self.ffn = FFN(d_model=d_model, device=device)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, device =device)\n",
    "\n",
    "        self.device = device\n",
    "    def forward(self, input_emb, mask=None):\n",
    "        input_emb = input_emb.to(self.device)\n",
    "        x = self.mha(input_emb, input_emb, input_emb, mask)\n",
    "        x = self.layernorm1(x + input_emb)\n",
    "        ffn_out = self.ffn(x).to(self.device)\n",
    "        x = self.layernorm2(x + ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder block test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the encoder block\n",
    "def test_encoder_block(device):\n",
    "    num_heads = 8\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    encoder_block = Encoder(num_heads, d_model, device= device)\n",
    "\n",
    "    input_emb = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = encoder_block(input_emb)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"Encoder block test passed\")\n",
    "test_encoder_block(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Decoders' Implementation in the Transformer\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,num_heads, d_model, device, dropout=0.1 ):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(num_heads, d_model, dropout, device=device)\n",
    "        self.cross_attn = MultiHeadAttention(num_heads, d_model,dropout, device=device)\n",
    "\n",
    "        self.ffn = FFN(d_model =d_model, device=device)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model, device=device)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, device=device)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model, device=device)\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # Self-attention\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross-attention\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.layernorm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "        return self.layernorm3(x + self.dropout(ffn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Decoder' object has no attribute 'dropout'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m out\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m (batch_size, \u001b[38;5;241m10\u001b[39m, d_model)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoder block test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtest_decoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 12\u001b[0m, in \u001b[0;36mtest_decoder_block\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m      9\u001b[0m output_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m10\u001b[39m, d_model, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     10\u001b[0m residual \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, \u001b[38;5;241m10\u001b[39m, d_model, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 12\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m (batch_size, \u001b[38;5;241m10\u001b[39m, d_model)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoder block test passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rishabh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rishabh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[29], line 19\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, enc_output, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, enc_output, src_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tgt_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Self-attention\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x, tgt_mask)\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m(attn_output))\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Cross-attention\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(x, enc_output, enc_output, src_mask)\n",
      "File \u001b[1;32mc:\\Users\\Rishabh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Decoder' object has no attribute 'dropout'"
     ]
    }
   ],
   "source": [
    "# Test the decoder block\n",
    "def test_decoder_block(device):\n",
    "    h = 8\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    decoder_block = Decoder(h,  d_model, device=device)\n",
    "\n",
    "    output_emb = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    residual = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = decoder_block(output_emb, residual)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"Decoder block test passed\")\n",
    "test_decoder_block(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n, h, d_k, seq_len, d_model, device, sos_token=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sos_token = sos_token\n",
    "        self.embed1 = nn.Embedding(seq_len, d_model, device=device)\n",
    "        self.embed2 = nn.Embedding(seq_len, d_model, device=device)\n",
    "\n",
    "        self.n = n\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        self.encoder = nn.ModuleList([Encoder(h, d_k, d_model, device, dropout) for _ in range(n)])\n",
    "        self.decoder = nn.ModuleList([Decoder(h, d_k, d_model, device, dropout) for _ in range(n)])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model, device=device),\n",
    "        )\n",
    "\n",
    "    def shift_right(self, x):\n",
    "        \"\"\"\n",
    "        Shifts the input tensor to the right - removes the last element for prediction\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        batch_size, _, emb = x.shape\n",
    "        if self.sos_token is not None:\n",
    "            sos_column = torch.full((batch_size, 1), self.sos_token, dtype=x.dtype, device=x.device)\n",
    "        else:\n",
    "            sos_column = torch.zeros_like(x[:, :1, :])\n",
    "        shifted = torch.cat([sos_column, x[:, :-1, :]], dim=1) # gets 0 in the first column and removes the last column (token)\n",
    "        return shifted.to(self.device)\n",
    "    \n",
    "    def positional_embedding(self, input_emb, output_emb):\n",
    "        _, seq_len, _ = input_emb.shape  # Sequence length\n",
    "\n",
    "         # Create positional encoding matrix (seq_len, d_model)\n",
    "        log_10000 = torch.log(torch.tensor(10000.0))\n",
    "        position = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)  # Shape: (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float32) * -(log_10000 / self.d_model))  # Shape: (d_model // 2)\n",
    "\n",
    "        # Compute sine and cosine components\n",
    "        pos_enc = torch.zeros((seq_len, self.d_model), dtype=torch.float32)\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "\n",
    "        # Broadcast and add positional encoding to input and output embeddings\n",
    "        pos_enc = pos_enc.unsqueeze(0)  # Shape: (1, seq_len, d_model), ready for broadcasting\n",
    "        input_emb = input_emb + pos_enc.to(self.device)\n",
    "        output_emb = output_emb + pos_enc.to(self.device)\n",
    "        \n",
    "        return input_emb, output_emb\n",
    "    \n",
    "    def infer(self):\n",
    "        #TODO: needs to be changed to accomodate self.training for different inference methods\n",
    "        #TODO: figure out inference for transformer model - use of self.training\n",
    "        ...\n",
    "    \n",
    "    def forward(self, inp, out, targets =None):\n",
    "\n",
    "        if self.training:\n",
    "            input_emb = self.embed1(inp)\n",
    "\n",
    "            output_emb = self.shift_right(out)\n",
    "            output_emb = self.embed2(output_emb)\n",
    "        \n",
    "            input_emb, output_emb = self.positional_embedding(input_emb, output_emb) # apply embeddings\n",
    "            # pass through encoder and decoder stack\n",
    "            for i in range(self.n):\n",
    "                input_emb = self.encoder[i](input_emb)\n",
    "                output_emb = self.decoder[i](output_emb, input_emb)\n",
    "            logits = self.final(output_emb) # compute the probabilities\n",
    "            loss = F.cross_entropy(input=logits, target=targets) if targets is not None else None\n",
    "            # We do not apply softmax in the last layer as cross entropy expects raw logits \n",
    "            return logits , loss\n",
    "        else:\n",
    "            self.infer() #TODO: to be implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
