{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the Todo-list\n",
    "Will be updated as we go about the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Test the encoder and the decoder\n",
    "#TODO: implement the model\n",
    "#TODO: look into torchtext\n",
    "#TODO: get the data\n",
    "#TODO: pre-process the data - tokenize it as well\n",
    "#TODO: implement the training loop\n",
    "#TODO: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Rishabh Agarwal\n",
    "# All the imports for the code\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "# Used in the Transformer Block\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model=512):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "        self.gelu = nn.GELU()\n",
    "        # One reason of choosing GELU - not in the initial implementation of the transformer in the paper\n",
    "        \"\"\"\n",
    "        relu can suffer from \"problems where significant amount of neuron in the network become zero and donâ€™t practically do anything.\" \n",
    "        gelu is smoother near zero and \"is differentiable in all ranges, and allows to have gradients(although small) in negative range\" which helps with this problem.\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiheaded attention class for the transformer\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_k, d_model, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.d_k = d_k # d_k = d_v\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        assert d_model % h == 0  # Assert that the number of heads divides the model dimension\n",
    "        assert d_k == d_model // h # Assert that the key and value dimensions are equal to d_model // h\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k, device = device)\n",
    "        self.W_K = nn.Linear(d_model, d_k, device = device)\n",
    "        self.W_V = nn.Linear(d_model, d_k, device = device)\n",
    "\n",
    "        self.W_O = nn.Linear(h*d_k, d_model, device = device)\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: (batch_size, seq_len, d_model)\n",
    "        # mask: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        concatenated_heads = torch.tensor([], device=self.device)\n",
    "\n",
    "        # The multiple heads being computed to be concatenated together and then applied to the output layer\n",
    "        # can be done in parallel\n",
    "        for _ in range(self.h):\n",
    "            # Linearly project the queries\n",
    "            computed_Q = self.W_Q(Q)\n",
    "            # Linearly project the keys\n",
    "            computed_K = self.W_K(K)\n",
    "            # Linearly project the queries, keys, and values\n",
    "            computed_V = self.W_V(V)\n",
    "\n",
    "            # Calculate the attention scores\n",
    "            head_i = F.scaled_dot_product_attention(computed_Q, computed_K, computed_V, mask, dropout_p=self.dropout)\n",
    "            # print(head_i.shape)\n",
    "            concatenated_heads = torch.cat((concatenated_heads, head_i), dim=-1)\n",
    "            \n",
    "        return self.W_O(concatenated_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the MultiHeadedAttention class\n",
    "def test_multi_headed_attention(device):\n",
    "    h = 8\n",
    "    d_k = 64\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    mha = MultiHeadedAttention(h, d_k, d_model,device)\n",
    "\n",
    "    Q = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    K = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    V = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = mha(Q, K, V)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"MultiHeadedAttention test passed\")\n",
    "test_multi_headed_attention(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Encoder Blocks in the Transformer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, h, d_k, d_model, device, dropout=0.1):\n",
    "        self.mha = MultiHeadedAttention(h, d_k, d_model, device, dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.ffn = FFN(d_model)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.device = device\n",
    "    def forward(self, input_emb):\n",
    "        input_emb = input_emb.to(self.device)\n",
    "        x = self.mha(input_emb, input_emb, input_emb).to(self.device)\n",
    "        x = self.layernorm1(x + input_emb).to(self.device)\n",
    "        ffn_out = self.ffn(x).to(self.device)\n",
    "        x = self.layernorm2(x + ffn_out).to(self.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Decoders' Implementation in the Transformer\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,h, d_k, d_model, device, dropout=0.1 ):\n",
    "        self.masked_mha = MultiHeadedAttention(h, d_k, d_model, device, dropout)\n",
    "        self.mha = MultiHeadedAttention(h, d_k, d_model, device, dropout)\n",
    "\n",
    "        self.ffn = FFN(d_model)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.device = device\n",
    "    \n",
    "    \n",
    "    def mask(self, x):\n",
    "        \"\"\"\n",
    "        Mask the output embeddings\n",
    "        \"\"\"\n",
    "        batch, seq, emb = x.shape\n",
    "\n",
    "        mask = torch.tril(torch.ones(seq, seq), diagonal=1) == 0\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        mask = mask.to(self.device)\n",
    "\n",
    "        return (x+mask).to(self.device)\n",
    "    def forward(self, output_emb, residual):\n",
    "        \"\"\"\n",
    "        Residual connection comes from encoder\n",
    "        \"\"\"\n",
    "\n",
    "        assert output_emb.shape == residual.shape\n",
    "\n",
    "        mask = self.mask(output_emb)\n",
    "        x = self.masked_mha(output_emb, output_emb, output_emb, mask).to(self.device)\n",
    "        x = self.layernorm1(output_emb + x).to(self.device)\n",
    "\n",
    "        y = self.mha(residual, residual, x).to(self.device)\n",
    "        y = self.layernorm2(x + y).to(self.device)\n",
    "\n",
    "        ffn_out = self.ffn(y).to(self.device)\n",
    "        output = self.layernorm3(y + ffn_out).to(self.device)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n, h, d_k, d_model, device, dropout=0.1):\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Softmax(dim =-1),\n",
    "        )\n",
    "    def forward(self, input_emb, output_emb):\n",
    "        ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
