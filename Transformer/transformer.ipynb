{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the Todo-list\n",
    "Will be updated as we go about the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: look into torchtext\n",
    "#TODO: get the data\n",
    "#TODO: pre-process the data - tokenize it as well\n",
    "#TODO: implement the training loop\n",
    "#TODO: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Rishabh Agarwal\n",
    "# All the imports for the code\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "# Used in the Transformer Block\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, device, d_model=512):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4, device=device)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model, device=device)\n",
    "        self.gelu = nn.GELU()\n",
    "        # One reason of choosing GELU - not in the initial implementation of the transformer in the paper\n",
    "        \"\"\"\n",
    "        relu can suffer from \"problems where significant amount of neuron in the network become zero and donâ€™t practically do anything.\" \n",
    "        gelu is smoother near zero and \"is differentiable in all ranges, and allows to have gradients(although small) in negative range\" which helps with this problem.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiheaded attention class for the transformer\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_k, d_model, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.d_k = d_k # d_k = d_v\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        assert d_model % h == 0  # Assert that the number of heads divides the model dimension\n",
    "        assert d_k == d_model // h # Assert that the key and value dimensions are equal to d_model // h\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k, device = device)\n",
    "        self.W_K = nn.Linear(d_model, d_k, device = device)\n",
    "        self.W_V = nn.Linear(d_model, d_k, device = device)\n",
    "\n",
    "        self.W_O = nn.Linear(h*d_k, d_model, device = device)\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: (batch_size, seq_len, d_model)\n",
    "        # mask: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        concatenated_heads = torch.tensor([], device=self.device)\n",
    "\n",
    "        # The multiple heads being computed to be concatenated together and then applied to the output layer\n",
    "        # can be done in parallel\n",
    "        for _ in range(self.h):\n",
    "            # Linearly project the queries\n",
    "            computed_Q = self.W_Q(Q)\n",
    "            # Linearly project the keys\n",
    "            computed_K = self.W_K(K)\n",
    "            # Linearly project the queries, keys, and values\n",
    "            computed_V = self.W_V(V)\n",
    "\n",
    "            # Calculate the attention scores\n",
    "            head_i = F.scaled_dot_product_attention(computed_Q, computed_K, computed_V, mask, dropout_p=self.dropout)\n",
    "            # print(head_i.shape)\n",
    "            concatenated_heads = torch.cat((concatenated_heads, head_i), dim=-1)\n",
    "            \n",
    "        return self.W_O(concatenated_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention test passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishabh\\AppData\\Local\\Temp\\ipykernel_432292\\1934682931.py:42: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  head_i = F.scaled_dot_product_attention(computed_Q, computed_K, computed_V, mask, dropout_p=self.dropout)\n"
     ]
    }
   ],
   "source": [
    "# Test the MultiHeadedAttention class\n",
    "def test_multi_headed_attention(device):\n",
    "    h = 8\n",
    "    d_k = 64\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    mha = MultiHeadedAttention(h, d_k, d_model,device)\n",
    "\n",
    "    Q = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    K = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    V = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = mha(Q, K, V)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"MultiHeadedAttention test passed\")\n",
    "test_multi_headed_attention(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Encoder Blocks in the Transformer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, h, d_k, d_model, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadedAttention(h, d_k, d_model, device, dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model, device=device)\n",
    "\n",
    "        self.ffn = FFN(d_model=d_model, device=device)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, device =device)\n",
    "\n",
    "        self.device = device\n",
    "    def forward(self, input_emb):\n",
    "        input_emb = input_emb.to(self.device)\n",
    "        x = self.mha(input_emb, input_emb, input_emb)\n",
    "        x = self.layernorm1(x + input_emb)\n",
    "        ffn_out = self.ffn(x).to(self.device)\n",
    "        x = self.layernorm2(x + ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder block test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the encoder block\n",
    "def test_encoder_block(device):\n",
    "    h = 8\n",
    "    d_k = 64\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    encoder_block = Encoder(h, d_k, d_model, device)\n",
    "\n",
    "    input_emb = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = encoder_block(input_emb)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"Encoder block test passed\")\n",
    "test_encoder_block(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Decoders' Implementation in the Transformer\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,h, d_k, d_model, device, dropout=0.1 ):\n",
    "        super().__init__()\n",
    "        self.masked_mha = MultiHeadedAttention(h, d_k, d_model, device, dropout)\n",
    "        self.mha = MultiHeadedAttention(h, d_k, d_model, device, dropout)\n",
    "\n",
    "        self.ffn = FFN(d_model =d_model, device=device)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model, device=device)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, device=device)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model, device=device)\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    def mask(self, x):\n",
    "        \"\"\"\n",
    "        Mask the output embeddings\n",
    "        \"\"\"\n",
    "        batch, seq, emb = x.shape\n",
    "\n",
    "        mask = torch.tril(torch.ones(batch,seq, seq), diagonal=1) == 0\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        mask = mask.to(self.device)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, output_emb, residual):\n",
    "        \"\"\"\n",
    "        Residual connection comes from encoder\n",
    "        \"\"\"\n",
    "\n",
    "        assert output_emb.shape == residual.shape\n",
    "\n",
    "        mask = self.mask(output_emb)\n",
    "        x = self.masked_mha(output_emb, output_emb, output_emb, mask).to(self.device)\n",
    "        x = self.layernorm1(output_emb + x).to(self.device)\n",
    "\n",
    "        y = self.mha(residual, residual, x).to(self.device)\n",
    "        y = self.layernorm2(x + y).to(self.device)\n",
    "\n",
    "        ffn_out = self.ffn(y).to(self.device)\n",
    "        output = self.layernorm3(y + ffn_out).to(self.device)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder block test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder block\n",
    "def test_decoder_block(device):\n",
    "    h = 8\n",
    "    d_k = 64\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    decoder_block = Decoder(h, d_k, d_model, device)\n",
    "\n",
    "    output_emb = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    residual = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = decoder_block(output_emb, residual)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"Decoder block test passed\")\n",
    "test_decoder_block(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n, h, d_k, d_model, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.encoder = nn.ModuleList([Encoder(h, d_k, d_model, device, dropout) for _ in range(n)])\n",
    "        self.decoder = nn.ModuleList([Decoder(h, d_k, d_model, device, dropout) for _ in range(n)])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model, device=device),\n",
    "            nn.Softmax(dim =-1),\n",
    "        )\n",
    "    def forward(self, input_emb, output_emb):\n",
    "        for i in range(self.n):\n",
    "            input_emb = self.encoder[i](input_emb)\n",
    "            output_emb = self.decoder[i](output_emb, input_emb)\n",
    "        out = self.final(output_emb)\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer test passed\n"
     ]
    }
   ],
   "source": [
    "# test transformer block\n",
    "def test_transformer(device):\n",
    "    n = 6\n",
    "    h = 8\n",
    "    d_k = 64\n",
    "    d_model = d_k*h\n",
    "\n",
    "\n",
    "    transformer = Transformer(n, h, d_k, d_model, device)\n",
    "\n",
    "    sample_input = torch.randn(32, 10, d_model, device=device) # positional encoding not added yet \n",
    "    sample_output = torch.randn(32, 10, d_model, device=device) # positional encoding not added yet \n",
    "\n",
    "    out = transformer(sample_input, sample_output)\n",
    "\n",
    "    assert out.size() == (32, 10, d_model)\n",
    "\n",
    "    print(\"Transformer test passed\")\n",
    "\n",
    "test_transformer(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
