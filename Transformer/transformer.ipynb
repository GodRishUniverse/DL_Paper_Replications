{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the Todo-list\n",
    "Will be updated as we go about the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: figure out inference for transformer model - use of self.training\n",
    "#TODO: look into torchtext\n",
    "#TODO: get the data\n",
    "#TODO: pre-process the data - tokenize it as well\n",
    "#TODO: implement the training loop\n",
    "#TODO: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Rishabh Agarwal\n",
    "# All the imports for the code\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "# Used in the Transformer Block\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, device, d_model=512, dff = 2048):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dff, device=device)\n",
    "        self.linear2 = nn.Linear(dff, d_model, device=device)\n",
    "        self.gelu = nn.GELU()\n",
    "        # One reason of choosing GELU - not in the initial implementation of the transformer in the paper\n",
    "        \"\"\"\n",
    "        relu can suffer from \"problems where significant amount of neuron in the network become zero and donâ€™t practically do anything.\" \n",
    "        gelu is smoother near zero and \"is differentiable in all ranges, and allows to have gradients(although small) in negative range\" which helps with this problem.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dropout=0.1, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.device = device\n",
    "        self.d_k = d_model // num_heads # using d_model and number of heads for d_k calculation\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Combined linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(d_model, d_model,device=self.device)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, device=self.device)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, device=self.device)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, device= self.device)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections and reshape for attention\n",
    "        q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_proj(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_proj(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention (can use internal implementation as well)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attn, v)\n",
    "        \n",
    "        # Reshape and apply output projection\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.out_proj(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the MultiHeadedAttention class\n",
    "def test_multi_headed_attention(device):\n",
    "    h = 8\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    mha = MultiHeadAttention(num_heads=h, d_model= d_model, device=device)\n",
    "\n",
    "    Q = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    K = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    V = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = mha(Q, K, V)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"MultiHeadedAttention test passed\")\n",
    "test_multi_headed_attention(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Encoder Blocks in the Transformer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, d_model, dff,  device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads, d_model, dropout, device=device)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model, device=device)\n",
    "\n",
    "        self.ffn = FFN(d_model=d_model, device=device, dff=dff)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, device =device)\n",
    "\n",
    "        self.device = device\n",
    "    def forward(self, input_emb, mask=None):\n",
    "        input_emb = input_emb.to(self.device)\n",
    "        x = self.mha(input_emb, input_emb, input_emb, mask)\n",
    "        x = self.layernorm1(x + input_emb)\n",
    "        ffn_out = self.ffn(x).to(self.device)\n",
    "        x = self.layernorm2(x + ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder block test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the encoder block\n",
    "def test_encoder_block(device):\n",
    "    num_heads = 8\n",
    "    d_model = 512\n",
    "    dff = 2048\n",
    "    batch_size = 32\n",
    "\n",
    "    encoder_block = Encoder(num_heads, d_model, dff= dff, device= device)\n",
    "\n",
    "    input_emb = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = encoder_block(input_emb)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"Encoder block test passed\")\n",
    "test_encoder_block(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Decoders' Implementation in the Transformer\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,num_heads, d_model, dff, device, dropout=0.1 ):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(num_heads, d_model, dropout, device=device)\n",
    "        self.cross_attn = MultiHeadAttention(num_heads, d_model,dropout, device=device)\n",
    "\n",
    "        self.ffn = FFN(d_model =d_model, device=device, dff= dff)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model, device=device)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, device=device)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model, device=device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, residual, src_mask=None, tgt_mask=None):\n",
    "        # Self-attention\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.layernorm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross-attention\n",
    "        attn_output = self.cross_attn(x, residual, residual, src_mask)\n",
    "        x = self.layernorm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "        return self.layernorm3(x + self.dropout(ffn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder block test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder block\n",
    "def test_decoder_block(device):\n",
    "    h = 8\n",
    "    d_model = 512\n",
    "    dff = 2048\n",
    "    batch_size = 32\n",
    "\n",
    "    decoder_block = Decoder(h,  d_model, device=device, dff=dff)\n",
    "\n",
    "    output_emb = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    residual = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = decoder_block(output_emb, residual)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"Decoder block test passed\")\n",
    "test_decoder_block(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, dropout=0.1,\n",
    "                 pad_idx=0, max_seq_len=1000, device='cuda'):\n",
    "        \"\"\"\n",
    "        \n",
    "        src stands for source/input\n",
    "        tgt stands for target/output\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.pad_index = pad_idx\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "        self.embed1 = nn.Embedding(src_vocab_size, d_model, device=device)\n",
    "        self.embed2 = nn.Embedding(tgt_vocab_size, d_model, device=device)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_encoding = self.create_positional_encoding(max_seq_len, d_model)  # Max sequence length of 1000\n",
    "        # rather than calculating this every forward pass we calculate this once and store it\n",
    "\n",
    "        \n",
    "        self.encoder = nn.ModuleList([Encoder(num_heads, d_model, dropout, device=device, dff=d_ff) for _ in range(num_encoder_layers)])\n",
    "        self.decoder = nn.ModuleList([Decoder(num_heads, d_model, dropout, device=device, dff=d_ff) for _ in range(num_decoder_layers)])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(d_model, tgt_vocab_size, device=device),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def create_positional_encoding(self, max_seq_len, d_model):\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def create_padding_mask(self, src):\n",
    "        return (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def create_causal_mask(self, tgt):\n",
    "        seq_len = tgt.size(1)\n",
    "        return torch.tril(torch.ones(seq_len, seq_len)).bool()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        # Utilizaing Xavier Initialization\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, input_emb, output_emb):\n",
    "\n",
    "        src_mask = self.create_padding_mask(input_emb)\n",
    "        tgt_mask = self.create_causal_mask(output_emb)\n",
    "        \n",
    "        # Embedding and positional encoding for source\n",
    "        src_embedded = self.src_embed(input_emb) * math.sqrt(self.d_model)\n",
    "        src_embedded = src_embedded + self.pos_encoding[:, :src_embedded.size(1)].to(src_embedded.device)\n",
    "        enc_output = self.dropout(src_embedded)\n",
    "\n",
    "        for enc in self.encoder:\n",
    "            enc_output = enc(enc_output, src_mask)\n",
    "        \n",
    "        # Embedding and positional encoding for target\n",
    "        tgt_embedded = self.tgt_embed(output_emb) * math.sqrt(self.d_model)\n",
    "        tgt_embedded = tgt_embedded + self.pos_encoding[:, :tgt_embedded.size(1)].to(tgt_embedded.device)\n",
    "        dec_output = self.dropout(tgt_embedded)\n",
    "\n",
    "        # Decoder\n",
    "        for dec in self.decoder:\n",
    "            dec_output = dec(x = dec_output, residual = enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \n",
    "        return self.final_layer(dec_output)\n",
    "    \n",
    "    def generate(self, inputs, max_len, sos_token, eos_token):\n",
    "        self.eval()\n",
    "        with torch.inference_mode():\n",
    "            # Encode source sequence\n",
    "            src_mask = self.create_padding_mask(inputs)\n",
    "            \n",
    "            src_embedded = self.src_embed(inputs) * math.sqrt(self.d_model)\n",
    "            src_embedded = src_embedded + self.pos_encoding[:, :src_embedded.size(1)].to(src_embedded.device)\n",
    "            enc_output = self.dropout(src_embedded)\n",
    "            \n",
    "            for enc in self.encoder:\n",
    "                enc_output = enc(enc_output, src_mask)\n",
    "            \n",
    "            # Initialize target sequence with SOS token\n",
    "            target = torch.full((inputs.size(0), 1), sos_token, dtype=torch.long, device=self.device)\n",
    "            \n",
    "            # Generate tokens one by one\n",
    "            for _ in range(max_len - 1):\n",
    "                tgt_mask = self.create_causal_mask(target)\n",
    "                \n",
    "                tgt_embedded = self.tgt_embed(target) * math.sqrt(self.d_model)\n",
    "                tgt_embedded = tgt_embedded + self.pos_encoding[:, :tgt_embedded.size(1)].to(tgt_embedded.device)\n",
    "                dec_output = self.dropout(tgt_embedded)\n",
    "                \n",
    "                for dec in self.decoder:\n",
    "                    dec_output = dec(x = dec_output, residual = enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "                \n",
    "                output = self.final_layer(dec_output)\n",
    "                next_token = output[:, -1].argmax(dim=-1).unsqueeze(1)\n",
    "                target = torch.cat([target, next_token], dim=1)\n",
    "                \n",
    "                # Stop if EOS token is generated\n",
    "                if next_token.item() == eos_token:\n",
    "                    break\n",
    "            \n",
    "            return target\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
