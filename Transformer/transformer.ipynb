{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer implementation Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the Todo-list\n",
    "Will be updated as we go about the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: figure out inference for transformer model - use of self.training\n",
    "#TODO: look into torchtext\n",
    "#TODO: get the data\n",
    "#TODO: pre-process the data - tokenize it as well\n",
    "#TODO: implement the training loop\n",
    "#TODO: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Rishabh Agarwal\n",
    "# All the imports for the code\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "# Used in the Transformer Block\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, device, d_model=512):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4, device=device)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model, device=device)\n",
    "        self.gelu = nn.GELU()\n",
    "        # One reason of choosing GELU - not in the initial implementation of the transformer in the paper\n",
    "        \"\"\"\n",
    "        relu can suffer from \"problems where significant amount of neuron in the network become zero and donâ€™t practically do anything.\" \n",
    "        gelu is smoother near zero and \"is differentiable in all ranges, and allows to have gradients(although small) in negative range\" which helps with this problem.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiheaded attention class for the transformer\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_k, d_model, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.d_k = d_k # d_k = d_v\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        assert d_model % h == 0  # Assert that the number of heads divides the model dimension\n",
    "        assert d_k == d_model // h # Assert that the key and value dimensions are equal to d_model // h\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_k, device = device)\n",
    "        self.W_K = nn.Linear(d_model, d_k, device = device)\n",
    "        self.W_V = nn.Linear(d_model, d_k, device = device)\n",
    "\n",
    "        self.W_O = nn.Linear(h*d_k, d_model, device = device)\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, V: (batch_size, seq_len, d_model)\n",
    "        # mask: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # The multiple heads being computed to be concatenated together and then applied to the output layer\n",
    "        # can be done in parallel\n",
    "        heads = []\n",
    "        for _ in range(self.h):\n",
    "            # Linearly project the queries\n",
    "            computed_Q = self.W_Q(Q)\n",
    "            # Linearly project the keys\n",
    "            computed_K = self.W_K(K)\n",
    "            # Linearly project the queries, keys, and values\n",
    "            computed_V = self.W_V(V)\n",
    "\n",
    "            # Calculate the attention scores\n",
    "            head_i = F.scaled_dot_product_attention(computed_Q, computed_K, computed_V, mask, dropout_p=self.dropout)\n",
    "            heads.append(head_i)\n",
    "        concatenated_heads = torch.cat(heads, dim=-1)\n",
    "            \n",
    "        return self.W_O(concatenated_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention test passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishabh\\AppData\\Local\\Temp\\ipykernel_432292\\1934682931.py:42: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  head_i = F.scaled_dot_product_attention(computed_Q, computed_K, computed_V, mask, dropout_p=self.dropout)\n"
     ]
    }
   ],
   "source": [
    "# Test the MultiHeadedAttention class\n",
    "def test_multi_headed_attention(device):\n",
    "    h = 8\n",
    "    d_k = 64\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    mha = MultiHeadedAttention(h, d_k, d_model,device)\n",
    "\n",
    "    Q = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    K = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    V = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = mha(Q, K, V)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"MultiHeadedAttention test passed\")\n",
    "test_multi_headed_attention(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Encoder Blocks in the Transformer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, h, d_k, d_model, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadedAttention(h, d_k, d_model, device, dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(d_model, device=device)\n",
    "\n",
    "        self.ffn = FFN(d_model=d_model, device=device)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, device =device)\n",
    "\n",
    "        self.device = device\n",
    "    def forward(self, input_emb):\n",
    "        input_emb = input_emb.to(self.device)\n",
    "        x = self.mha(input_emb, input_emb, input_emb)\n",
    "        x = self.layernorm1(x + input_emb)\n",
    "        ffn_out = self.ffn(x).to(self.device)\n",
    "        x = self.layernorm2(x + ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder block test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the encoder block\n",
    "def test_encoder_block(device):\n",
    "    h = 8\n",
    "    d_k = 64\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    encoder_block = Encoder(h, d_k, d_model, device)\n",
    "\n",
    "    input_emb = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = encoder_block(input_emb)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"Encoder block test passed\")\n",
    "test_encoder_block(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of N Decoders' Implementation in the Transformer\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,h, d_k, d_model, device, dropout=0.1 ):\n",
    "        super().__init__()\n",
    "        self.masked_mha = MultiHeadedAttention(h, d_k, d_model, device, dropout)\n",
    "        self.mha = MultiHeadedAttention(h, d_k, d_model, device, dropout)\n",
    "\n",
    "        self.ffn = FFN(d_model =d_model, device=device)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model, device=device)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model, device=device)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model, device=device)\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "    def mask(self, x):\n",
    "        \"\"\"\n",
    "        Mask the output embeddings\n",
    "        \"\"\"\n",
    "        batch, seq, emb = x.shape\n",
    "\n",
    "        mask = torch.tril(torch.ones(batch,seq, seq), diagonal=1) == 0\n",
    "        #TODO: Neec to verify if we want the mask to block only future tokens. If so, might use diagonal=0 \n",
    "        mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        mask = mask.to(self.device)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, output_emb, residual):\n",
    "        \"\"\"\n",
    "        Residual connection comes from encoder\n",
    "        \"\"\"\n",
    "\n",
    "        assert output_emb.shape == residual.shape\n",
    "\n",
    "        mask = self.mask(output_emb)\n",
    "        x = self.masked_mha(output_emb, output_emb, output_emb, mask).to(self.device)\n",
    "        x = self.layernorm1(output_emb + x).to(self.device)\n",
    "\n",
    "        y = self.mha(residual, residual, x).to(self.device)\n",
    "        y = self.layernorm2(x + y).to(self.device)\n",
    "\n",
    "        ffn_out = self.ffn(y).to(self.device)\n",
    "        output = self.layernorm3(y + ffn_out).to(self.device)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder block test passed\n"
     ]
    }
   ],
   "source": [
    "# Test the decoder block\n",
    "def test_decoder_block(device):\n",
    "    h = 8\n",
    "    d_k = 64\n",
    "    d_model = 512\n",
    "    batch_size = 32\n",
    "\n",
    "    decoder_block = Decoder(h, d_k, d_model, device)\n",
    "\n",
    "    output_emb = torch.randn(batch_size, 10, d_model, device=device)\n",
    "    residual = torch.randn(batch_size, 10, d_model, device=device)\n",
    "\n",
    "    out = decoder_block(output_emb, residual)\n",
    "\n",
    "    assert out.size() == (batch_size, 10, d_model)\n",
    "\n",
    "    print(\"Decoder block test passed\")\n",
    "test_decoder_block(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n, h, d_k, seq_len, d_model, device, sos_token=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.sos_token = sos_token\n",
    "        self.embed1 = nn.Embedding(seq_len, d_model, device=device)\n",
    "        self.embed2 = nn.Embedding(seq_len, d_model, device=device)\n",
    "\n",
    "        self.n = n\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        self.encoder = nn.ModuleList([Encoder(h, d_k, d_model, device, dropout) for _ in range(n)])\n",
    "        self.decoder = nn.ModuleList([Decoder(h, d_k, d_model, device, dropout) for _ in range(n)])\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model, device=device),\n",
    "        )\n",
    "\n",
    "    def shift_right(self, x):\n",
    "        \"\"\"\n",
    "        Shifts the input tensor to the right - removes the last element for prediction\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        batch_size, _, emb = x.shape\n",
    "        if self.sos_token is not None:\n",
    "            sos_column = torch.full((batch_size, 1), self.sos_token, dtype=x.dtype, device=x.device)\n",
    "        else:\n",
    "            sos_column = torch.zeros_like(x[:, :1, :])\n",
    "        shifted = torch.cat([sos_column, x[:, :-1, :]], dim=1) # gets 0 in the first column and removes the last column (token)\n",
    "        return shifted.to(self.device)\n",
    "    \n",
    "    def positional_embedding(self, input_emb, output_emb):\n",
    "        _, seq_len, _ = input_emb.shape  # Sequence length\n",
    "\n",
    "         # Create positional encoding matrix (seq_len, d_model)\n",
    "        log_10000 = torch.log(torch.tensor(10000.0))\n",
    "        position = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)  # Shape: (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float32) * -(log_10000 / self.d_model))  # Shape: (d_model // 2)\n",
    "\n",
    "        # Compute sine and cosine components\n",
    "        pos_enc = torch.zeros((seq_len, self.d_model), dtype=torch.float32)\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "\n",
    "        # Broadcast and add positional encoding to input and output embeddings\n",
    "        pos_enc = pos_enc.unsqueeze(0)  # Shape: (1, seq_len, d_model), ready for broadcasting\n",
    "        input_emb = input_emb + pos_enc.to(self.device)\n",
    "        output_emb = output_emb + pos_enc.to(self.device)\n",
    "        \n",
    "        return input_emb, output_emb\n",
    "    \n",
    "    def infer(self):\n",
    "        #TODO: needs to be changed to accomodate self.training for different inference methods\n",
    "        #TODO: figure out inference for transformer model - use of self.training\n",
    "        ...\n",
    "    \n",
    "    def forward(self, inp, out, targets =None):\n",
    "\n",
    "        if self.training:\n",
    "            input_emb = self.embed1(inp)\n",
    "\n",
    "            output_emb = self.shift_right(out)\n",
    "            output_emb = self.embed2(output_emb)\n",
    "        \n",
    "            input_emb, output_emb = self.positional_embedding(input_emb, output_emb) # apply embeddings\n",
    "            # pass through encoder and decoder stack\n",
    "            for i in range(self.n):\n",
    "                input_emb = self.encoder[i](input_emb)\n",
    "                output_emb = self.decoder[i](output_emb, input_emb)\n",
    "            logits = self.final(output_emb) # compute the probabilities\n",
    "            loss = F.cross_entropy(input=logits, target=targets) if targets is not None else None\n",
    "            # We do not apply softmax in the last layer as cross entropy expects raw logits \n",
    "            return logits , loss\n",
    "        else:\n",
    "            self.infer() #TODO: to be implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
